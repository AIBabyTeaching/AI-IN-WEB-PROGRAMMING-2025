{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f3af1c",
   "metadata": {},
   "source": [
    "# Lab 10 Â· Evaluation, Latency, and Cache\n",
    "\n",
    "*This lab notebook provides guided steps. All commands are intended for local execution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcd87f",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- A tiny evaluation set is logged for backend prompts.\n",
    "- Latency and token usage metrics are recorded.\n",
    "- A naive cache hashes prompt plus tool context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37ba39",
   "metadata": {},
   "source": [
    "## What will be learned\n",
    "- Evaluation harness design is reviewed for LLM flows.\n",
    "- Latency tracking is reinforced with timestamp instrumentation.\n",
    "- Cache strategies are described for simple reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717ec4b",
   "metadata": {},
   "source": [
    "## Prerequisites & install\n",
    "The following commands are intended for local execution.\n",
    "\n",
    "```bash\n",
    "cd ai-web/backend\n",
    ". .venv/bin/activate\n",
    "pip install google-generativeai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592d318",
   "metadata": {},
   "source": [
    "## Step-by-step tasks\n",
    "### Step 1: Evaluation dataset stub\n",
    "A small evaluation dataset is stored for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8263a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "fixture_path = Path(\"ai-web/backend/app/eval_set.json\")\n",
    "fixture_path.write_text('''[\n",
    "  {\"prompt\": \"Summarize the project goals.\", \"expected\": \"A concise description is returned.\"},\n",
    "  {\"prompt\": \"List backend components.\", \"expected\": \"FastAPI, FAISS, and Gemini proxy are noted.\"}\n",
    "]\n",
    "''')\n",
    "print(\"Evaluation dataset was created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700bdc7",
   "metadata": {},
   "source": [
    "### Step 2: Metrics helper\n",
    "A helper is provided to time requests and count tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ef914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "metrics_path = Path(\"ai-web/backend/app/metrics.py\")\n",
    "metrics_path.write_text('''import hashlib\n",
    "import json\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict\n",
    "\n",
    "TOKEN_LOG: Dict[str, int] = {}\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def track_latency(label: str):\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    duration = time.perf_counter() - start\n",
    "    print(f\"{label} latency: {duration:.3f}s\")\n",
    "\n",
    "\n",
    "def cache_key(prompt: str, tools: str) -> str:\n",
    "    payload = json.dumps({\"prompt\": prompt, \"tools\": tools}, sort_keys=True)\n",
    "    return hashlib.sha256(payload.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def record_tokens(label: str, count: int):\n",
    "    TOKEN_LOG[label] = TOKEN_LOG.get(label, 0) + count\n",
    "''')\n",
    "print(\"Metrics helper was written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f273dde",
   "metadata": {},
   "source": [
    "### Step 3: Cache-enabled endpoint\n",
    "An evaluation endpoint is instrumented with caching and latency tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "main_path = Path(\"ai-web/backend/app/main.py\")\n",
    "text = main_path.read_text()\n",
    "if \"evaluation_endpoint\" not in text:\n",
    "    addition = '''\n",
    "from .metrics import cache_key, record_tokens, track_latency\n",
    "from .llm import chat as llm_chat\n",
    "\n",
    "CACHE = {}\n",
    "\n",
    "\n",
    "@app.post(\"/api/evaluate\")\n",
    "def evaluation_endpoint(payload: dict):\n",
    "    prompt = payload.get(\"prompt\", \"\")\n",
    "    tools = \"\".join(payload.get(\"tools\", []))\n",
    "    key = cache_key(prompt, tools)\n",
    "    if key in CACHE:\n",
    "        return {\"cached\": True, \"response\": CACHE[key]}\n",
    "    with track_latency(\"evaluation\"):\n",
    "        response = llm_chat([\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "    record_tokens(\"evaluation\", len(prompt.split()))\n",
    "    CACHE[key] = response\n",
    "    return {\"cached\": False, \"response\": response}\n",
    "'''\n",
    "    main_path.write_text(text.rstrip() + \"\n",
    "\" + addition)\n",
    "    print(\"Evaluation endpoint was appended.\")\n",
    "else:\n",
    "    print(\"Evaluation endpoint already present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24aa74",
   "metadata": {},
   "source": [
    "## Validation / acceptance checks\n",
    "```bash\n",
    "# locally\n",
    "curl -X POST http://localhost:8000/api/evaluate -H 'Content-Type: application/json' -d '{\"prompt\":\"Summarize the project goals.\"}'\n",
    "```\n",
    "- Responses indicate whether cache hits occurred and include evaluation output.\n",
    "- React development mode shows the described UI state without console errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518080e1",
   "metadata": {},
   "source": [
    "## Homework / extensions\n",
    "- Token accounting is integrated with external monitoring dashboards.\n",
    "- Additional evaluation prompts are composed for regression coverage."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
