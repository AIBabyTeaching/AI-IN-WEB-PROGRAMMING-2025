{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017bf5e3",
   "metadata": {},
   "source": [
    "# Lab 03 · Gemini Proxy Chat\n",
    "\n",
    "*This lab notebook provides guided steps. All commands are intended for local execution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec0a65",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- A Gemini proxy endpoint is created in FastAPI.\n",
    "- React chat UI components are connected to the proxy.\n",
    "- API keys remain confined to the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f0d61",
   "metadata": {},
   "source": [
    "## What will be learned\n",
    "- Backend proxy patterns for hosted LLMs are practiced.\n",
    "- Basic chat state management in React is reviewed.\n",
    "- Environment variable usage is reinforced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059f46d",
   "metadata": {},
   "source": [
    "## Prerequisites & install\n",
    "The following commands are intended for local execution.\n",
    "\n",
    "```bash\n",
    "cd ai-web/backend\n",
    ". .venv/bin/activate\n",
    "pip install google-generativeai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9946c",
   "metadata": {},
   "source": [
    "## Step-by-step tasks\n",
    "### Step 1: Tour the Gemini service module\n",
    "\n",
    "Open `ai-web/backend/app/services/gemini.py` and guide students through the\n",
    "implementation. The file is heavily commented so you can read it nearly line by\n",
    "line:\n",
    "\n",
    "```python\n",
    "\"\"\"Service helpers for Gemini-powered features exposed by the FastAPI app.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import List\n",
    "```\n",
    "\n",
    "- The module docstring reminds instructors that all Gemini logic should live in\n",
    "  the service layer.\n",
    "- `from __future__ import annotations` keeps type hints as strings at runtime so\n",
    "  FastAPI imports stay lightweight.\n",
    "- Standard-library imports (`os`, `lru_cache`, `typing.List`) support\n",
    "  configuration, caching, and parsing.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except ImportError as exc:\n",
    "    genai = None\n",
    "    _IMPORT_ERROR = exc\n",
    "else:\n",
    "    _IMPORT_ERROR = None\n",
    "```\n",
    "\n",
    "- The `try/except` block lets unit tests run without the optional SDK. If the\n",
    "  import fails, the service stores the exception and raises a friendly error when\n",
    "  called.\n",
    "\n",
    "```python\n",
    "class GeminiServiceError(RuntimeError):\n",
    "    \"\"\"Raised when the Gemini helper cannot fulfill a request.\"\"\"\n",
    "```\n",
    "\n",
    "- Custom exceptions make it easy for routers to translate domain failures into\n",
    "  HTTP 503 responses.\n",
    "\n",
    "```python\n",
    "def _require_api_key() -> str:\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise GeminiServiceError(\n",
    "            \"GEMINI_API_KEY is not configured. Add it to backend/.env before calling the service.\"\n",
    "        )\n",
    "    return api_key\n",
    "```\n",
    "\n",
    "- `_require_api_key` centralizes validation so every caller gets the same error\n",
    "  message if the environment is missing.\n",
    "\n",
    "```python\n",
    "@lru_cache(maxsize=1)\n",
    "def _configure_client(api_key: str) -> bool:\n",
    "    if genai is None:\n",
    "        raise GeminiServiceError(\n",
    "            \"google-generativeai is not installed. Run `pip install google-generativeai` to enable the feature.\"\n",
    "        ) from _IMPORT_ERROR\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "    return True\n",
    "```\n",
    "\n",
    "- `@lru_cache` ensures the expensive `genai.configure` call only runs once per\n",
    "  process. The guard re-raises the original import error with additional context.\n",
    "\n",
    "```python\n",
    "def _parse_outline_lines(raw_outline: str) -> List[str]:\n",
    "    lines: List[str] = []\n",
    "    for line in raw_outline.splitlines():\n",
    "        cleaned = line.strip()\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        cleaned = cleaned.lstrip(\"-*•0123456789. \\t\")\n",
    "        if cleaned:\n",
    "            lines.append(cleaned)\n",
    "    return lines\n",
    "```\n",
    "\n",
    "- `_parse_outline_lines` sanitizes model output by trimming blank lines and\n",
    "  stripping bullets/numbers so the frontend receives plain text.\n",
    "\n",
    "```python\n",
    "def generate_lesson_outline(topic: str, model: str | None = None) -> dict[str, str | list[str]]:\n",
    "    normalized_topic = topic.strip()\n",
    "    if not normalized_topic:\n",
    "        raise ValueError(\"Topic must not be empty.\")\n",
    "\n",
    "    api_key = _require_api_key()\n",
    "    _configure_client(api_key)\n",
    "\n",
    "    selected_model = model or os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\")\n",
    "    try:\n",
    "        generative_model = genai.GenerativeModel(selected_model)\n",
    "        prompt = (\n",
    "            \"You are helping an instructor design a web programming lesson. \"\n",
    "            \"Return a concise outline with 3-5 bullet points that cover the key \"\n",
    "            \"concepts for the topic: \"\n",
    "            f\"{normalized_topic}.\"\n",
    "        )\n",
    "        response = generative_model.generate_content(prompt)\n",
    "        outline_text = getattr(response, \"text\", \"\").strip()\n",
    "    except Exception as exc:\n",
    "        raw_message = str(exc).strip()\n",
    "        if raw_message:\n",
    "            detail = f\": {raw_message}\"\n",
    "        else:\n",
    "            fallback = exc.__class__.__name__\n",
    "            detail = f\": {fallback}\"\n",
    "        raise GeminiServiceError(\n",
    "            f\"Failed to generate lesson outline{detail}.\"\n",
    "        ) from exc\n",
    "\n",
    "    outline = _parse_outline_lines(outline_text) if outline_text else []\n",
    "    return {\"topic\": normalized_topic, \"outline\": outline}\n",
    "```\n",
    "\n",
    "- The public function handles validation, client configuration, prompt creation,\n",
    "  and error translation. Use this walkthrough to emphasize how services encapsulate\n",
    "  third-party SDK calls while returning simple dictionaries to routers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "module = Path(\"ai-web/backend/app/llm.py\")\n",
    "module.parent.mkdir(parents=True, exist_ok=True)\n",
    "module.write_text('''import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from google import genai\n",
    "\n",
    "\n",
    "def chat(messages: List[Dict[str, Any]]) -> str:\n",
    "  api_key = os.environ.get('GEMINI_API_KEY', '')\n",
    "  if not api_key:\n",
    "    raise RuntimeError('A backend API key is required.')\n",
    "  client = genai.Client(api_key=api_key)\n",
    "  response = client.models.generate_content(\n",
    "      model=\"gemini-1.5-flash\",\n",
    "      contents=messages,\n",
    "  )\n",
    "  return response.text\n",
    "''')\n",
    "print(\"Gemini helper was written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6298a58",
   "metadata": {},
   "source": [
    "### Step 2: Review the Gemini router\n",
    "\n",
    "Next, inspect `ai-web/backend/app/routers/gemini.py` to see how the service is\n",
    "exposed to the frontend.\n",
    "\n",
    "```python\n",
    "router = APIRouter(prefix=\"/ai\", tags=[\"ai\"])\n",
    "```\n",
    "\n",
    "- Namespacing routes under `/ai` keeps AI endpoints grouped together for\n",
    "  documentation.\n",
    "\n",
    "```python\n",
    "class LessonOutlineIn(BaseModel):\n",
    "    topic: constr(strip_whitespace=True, min_length=1)\n",
    "```\n",
    "\n",
    "- Input validation mirrors the service’s `strip()` call so bad requests surface\n",
    "  as HTTP 422 responses before hitting Gemini.\n",
    "\n",
    "```python\n",
    "@router.post(\"/lesson-outline\", response_model=LessonOutlineOut)\n",
    "def lesson_outline(payload: LessonOutlineIn) -> LessonOutlineOut:\n",
    "    try:\n",
    "        result = generate_lesson_outline(payload.topic)\n",
    "    except ValueError as exc:\n",
    "        raise HTTPException(status_code=422, detail=str(exc)) from exc\n",
    "    except GeminiServiceError as exc:\n",
    "        logger.exception(\"Gemini lesson outline request failed\")\n",
    "        raise HTTPException(status_code=503, detail=str(exc)) from exc\n",
    "\n",
    "    return LessonOutlineOut(**result)\n",
    "```\n",
    "\n",
    "- The route delegates to the service and translates domain errors into HTTP\n",
    "  exceptions. Mention the `logger.exception` call so instructors remember to show\n",
    "  console logs when debugging with the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac05130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "main_path = Path(\"ai-web/backend/app/main.py\")\n",
    "text = main_path.read_text()\n",
    "addition = '''\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from .llm import chat as gemini_chat\n",
    "\n",
    "\n",
    "class ChatTurn(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: List[ChatTurn]\n",
    "\n",
    "\n",
    "@app.post(\"/api/chat\")\n",
    "def chat_endpoint(request: ChatRequest):\n",
    "    transcript = [\n",
    "        {\"role\": turn.role, \"parts\": [turn.content]} for turn in request.messages\n",
    "    ]\n",
    "    text = gemini_chat(transcript)\n",
    "    return {\"text\": text}\n",
    "'''\n",
    "if \"chat_endpoint\" not in text:\n",
    "    main_path.write_text(text.rstrip() + \"\n",
    "\" + addition)\n",
    "    print(\"FastAPI route was appended.\")\n",
    "else:\n",
    "    print(\"FastAPI route already present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3524459f",
   "metadata": {},
   "source": [
    "### Step 3: Demonstrate the Gemini lesson-outline feature\n",
    "\n",
    "Show how the frontend consumes the new endpoint:\n",
    "\n",
    "1. **Hook (`features/gemini/hooks/useLessonOutlineForm.js`)** – Posts the topic\n",
    "   to `/ai/lesson-outline`, manages loading/error state, and renders any structured\n",
    "   outline returned by the backend. Compare its structure to `useEchoForm` so the\n",
    "   shared pattern is obvious.\n",
    "2. **Component (`features/gemini/components/LessonOutlineForm.jsx`)** – Presents\n",
    "   the instructional copy, form fields, and formatted outline. Highlight how the\n",
    "   component reads `form.outline` to create a bulleted list.\n",
    "3. **App shell (`src/App.jsx`)** – Already renders the Gemini section under the\n",
    "   heading “Gemini lesson outline builder.” Reinforce that all AI features should\n",
    "   follow this composition model.\n",
    "\n",
    "Encourage instructors to run the flow live, pointing out the loading indicator\n",
    "and error alert when the backend raises `GeminiServiceError` (for example, by\n",
    "commenting out `GEMINI_API_KEY`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ccfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "app_js = Path(\"ai-web/frontend/src/App.jsx\")\n",
    "app_js.write_text('''import React, { useState } from 'react';\n",
    "import { post } from './lib/api';\n",
    "import { withRetry } from './lib/retry';\n",
    "\n",
    "function App() {\n",
    "  const [messages, setMessages] = useState([{ role: 'user', content: 'Hello Gemini' }]);\n",
    "  const [input, setInput] = useState('');\n",
    "  const [loading, setLoading] = useState(false);\n",
    "  const [error, setError] = useState('');\n",
    "\n",
    "  async function handleSend(event) {\n",
    "    event.preventDefault();\n",
    "    const updated = [...messages, { role: 'user', content: input }];\n",
    "    setMessages(updated);\n",
    "    setLoading(true);\n",
    "    setError('');\n",
    "    try {\n",
    "      const result = await withRetry(\n",
    "        () => post('/api/chat', { messages: updated }),\n",
    "        1,\n",
    "        800\n",
    "      );\n",
    "      setMessages([...updated, { role: 'model', content: result.text }]);\n",
    "      setInput('');\n",
    "    } catch (err) {\n",
    "      setError('The proxy was unreachable. Please try again.');\n",
    "    } finally {\n",
    "      setLoading(false);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return (\n",
    "    <main style={{ padding: 24 }}>\n",
    "      <h1>Lab 3 — Gemini proxy chat</h1>\n",
    "      <section>\n",
    "        {messages.map((msg, index) => (\n",
    "          <p key={index}>\n",
    "            <strong>{msg.role}:</strong> {msg.content}\n",
    "          </p>\n",
    "        ))}\n",
    "      </section>\n",
    "      <form onSubmit={handleSend}>\n",
    "        <input\n",
    "          value={input}\n",
    "          onChange={(event) => setInput(event.target.value)}\n",
    "          placeholder=\"Type a follow-up\"\n",
    "        />\n",
    "        <button type=\"submit\" disabled={loading || !input}>Send</button>\n",
    "      </form>\n",
    "      {loading && <p>Awaiting proxy response…</p>}\n",
    "      {error && <p style={{ color: 'red' }}>{error}</p>}\n",
    "    </main>\n",
    "  );\n",
    "}\n",
    "\n",
    "export default App;\n",
    "''')\n",
    "print(\"Chat UI was seeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361425b3",
   "metadata": {},
   "source": [
    "## Validation / acceptance checks\n",
    "```bash\n",
    "# locally\n",
    "export GEMINI_API_KEY=\"<your real key>\"\n",
    "uvicorn app.main:app --reload\n",
    "curl -X POST http://localhost:8000/ai/lesson-outline   -H 'Content-Type: application/json'   -d '{\"topic\":\"State management in React\"}'\n",
    "```\n",
    "- Response JSON includes the normalized topic and an `outline` array.\n",
    "- Frontend form displays bullet points that match the backend payload.\n",
    "- If `GEMINI_API_KEY` or the SDK import is missing, the API responds with HTTP 503\n",
    "  and surfaces the descriptive error message from `GeminiServiceError`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc9759",
   "metadata": {},
   "source": [
    "## Homework / extensions\n",
    "- Streaming responses are researched for future enhancements.\n",
    "- Chat history persistence is sketched for the next lab."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
