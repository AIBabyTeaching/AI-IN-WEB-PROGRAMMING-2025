{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017bf5e3",
   "metadata": {},
   "source": [
    "# Lab 03 · Gemini Proxy Chat\n",
    "\n",
    "*This lab notebook provides guided steps. All commands are intended for local execution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec0a65",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- A Gemini proxy endpoint is created in FastAPI.\n",
    "- React chat UI components are connected to the proxy.\n",
    "- API keys remain confined to the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f0d61",
   "metadata": {},
   "source": [
    "## What will be learned\n",
    "- Backend proxy patterns for hosted LLMs are practiced.\n",
    "- Basic chat state management in React is reviewed.\n",
    "- Environment variable usage is reinforced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059f46d",
   "metadata": {},
   "source": [
    "## Prerequisites & install\n",
    "The following commands are intended for local execution.\n",
    "\n",
    "```bash\n",
    "cd ai-web/backend\n",
    ". .venv/bin/activate\n",
    "pip install google-generativeai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9946c",
   "metadata": {},
   "source": [
    "## Step-by-step tasks\n",
    "### Step 1: Gemini helper module\n",
    "A backend helper is written so Gemini calls are centralized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "module = Path(\"ai-web/backend/app/llm.py\")\n",
    "module.parent.mkdir(parents=True, exist_ok=True)\n",
    "module.write_text('''import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from google import genai\n",
    "\n",
    "\n",
    "def chat(messages: List[Dict[str, Any]]) -> str:\n",
    "  api_key = os.environ.get('GEMINI_API_KEY', '')\n",
    "  if not api_key:\n",
    "    raise RuntimeError('A backend API key is required.')\n",
    "  client = genai.Client(api_key=api_key)\n",
    "  response = client.models.generate_content(\n",
    "      model=\"gemini-1.5-flash\",\n",
    "      contents=messages,\n",
    "  )\n",
    "  return response.text\n",
    "''')\n",
    "print(\"Gemini helper was written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6298a58",
   "metadata": {},
   "source": [
    "### Step 2: FastAPI route update\n",
    "The FastAPI app is expanded with /api/chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac05130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "main_path = Path(\"ai-web/backend/app/main.py\")\n",
    "text = main_path.read_text()\n",
    "addition = '''\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from .llm import chat as gemini_chat\n",
    "\n",
    "\n",
    "class ChatTurn(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: List[ChatTurn]\n",
    "\n",
    "\n",
    "@app.post(\"/api/chat\")\n",
    "def chat_endpoint(request: ChatRequest):\n",
    "    transcript = [\n",
    "        {\"role\": turn.role, \"parts\": [turn.content]} for turn in request.messages\n",
    "    ]\n",
    "    text = gemini_chat(transcript)\n",
    "    return {\"text\": text}\n",
    "'''\n",
    "if \"chat_endpoint\" not in text:\n",
    "    main_path.write_text(text.rstrip() + \"\n",
    "\" + addition)\n",
    "    print(\"FastAPI route was appended.\")\n",
    "else:\n",
    "    print(\"FastAPI route already present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3524459f",
   "metadata": {},
   "source": [
    "### Step 3: React chat surface\n",
    "A lightweight chat component is appended to App.jsx so the proxy is exercised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ccfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "app_js = Path(\"ai-web/frontend/src/App.jsx\")\n",
    "app_js.write_text('''import React, { useState } from 'react';\n",
    "import { post } from './lib/api';\n",
    "import { withRetry } from './lib/retry';\n",
    "\n",
    "function App() {\n",
    "  const [messages, setMessages] = useState([{ role: 'user', content: 'Hello Gemini' }]);\n",
    "  const [input, setInput] = useState('');\n",
    "  const [loading, setLoading] = useState(false);\n",
    "  const [error, setError] = useState('');\n",
    "\n",
    "  async function handleSend(event) {\n",
    "    event.preventDefault();\n",
    "    const updated = [...messages, { role: 'user', content: input }];\n",
    "    setMessages(updated);\n",
    "    setLoading(true);\n",
    "    setError('');\n",
    "    try {\n",
    "      const result = await withRetry(\n",
    "        () => post('/api/chat', { messages: updated }),\n",
    "        1,\n",
    "        800\n",
    "      );\n",
    "      setMessages([...updated, { role: 'model', content: result.text }]);\n",
    "      setInput('');\n",
    "    } catch (err) {\n",
    "      setError('The proxy was unreachable. Please try again.');\n",
    "    } finally {\n",
    "      setLoading(false);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return (\n",
    "    <main style={{ padding: 24 }}>\n",
    "      <h1>Lab 3 — Gemini proxy chat</h1>\n",
    "      <section>\n",
    "        {messages.map((msg, index) => (\n",
    "          <p key={index}>\n",
    "            <strong>{msg.role}:</strong> {msg.content}\n",
    "          </p>\n",
    "        ))}\n",
    "      </section>\n",
    "      <form onSubmit={handleSend}>\n",
    "        <input\n",
    "          value={input}\n",
    "          onChange={(event) => setInput(event.target.value)}\n",
    "          placeholder=\"Type a follow-up\"\n",
    "        />\n",
    "        <button type=\"submit\" disabled={loading || !input}>Send</button>\n",
    "      </form>\n",
    "      {loading && <p>Awaiting proxy response…</p>}\n",
    "      {error && <p style={{ color: 'red' }}>{error}</p>}\n",
    "    </main>\n",
    "  );\n",
    "}\n",
    "\n",
    "export default App;\n",
    "''')\n",
    "print(\"Chat UI was seeded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361425b3",
   "metadata": {},
   "source": [
    "## Validation / acceptance checks\n",
    "```bash\n",
    "# locally\n",
    "curl -X POST http://localhost:8000/api/chat -H 'Content-Type: application/json' -d '{\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}]}'\n",
    "```\n",
    "- A JSON response with a text field is produced by the backend proxy.\n",
    "- React development mode shows the described UI state without console errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc9759",
   "metadata": {},
   "source": [
    "## Homework / extensions\n",
    "- Streaming responses are researched for future enhancements.\n",
    "- Chat history persistence is sketched for the next lab."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
